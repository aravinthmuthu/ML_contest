#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 508
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize 12
\spacing single
\use_hyperref true
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder false
\pdf_colorlinks true
\pdf_backref false
\pdf_pdfusetitle true
\pdf_quoted_options "linkcolor=magenta, urlcolor=blue, citecolor=blue, pdfstartview={FitH}, hyperfootnotes=false, unicode=true"
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 1.5cm
\topmargin 1.5cm
\rightmargin 1.5cm
\bottommargin 1.5cm
\headheight 1.5cm
\headsep 1.5cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\listings_params "basicstyle={\ttfamily\small}"
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Report - ML Contest
\end_layout

\begin_layout Author
Shreyas Chaudhari - EE15B019
\begin_inset Newline newline
\end_inset

Aravinth Muthu - CS15B004
\end_layout

\begin_layout Part*
Introduction
\end_layout

\begin_layout Standard
Report for the Machine Learning contest hosted on Kaggle.
\end_layout

\begin_layout Subsection*
Understanding the data
\end_layout

\begin_layout Standard
The given dataset has 2600 features, with 9501 training samples and 29 classes.
 A few salient points about the data that clearly stand out are:
\end_layout

\begin_layout Itemize
The missing data is only from the first 500 features of both the test and
 train data.
 All other columns (features) have 0 missing samples
\end_layout

\begin_layout Itemize
The percentage of missing data is very high (around 40% in almost all features)
\end_layout

\begin_layout Itemize
The standard deviation of features the does not cross 0.35 for any feature
\end_layout

\begin_layout Standard
Based on these characteristics of data, we have selected our data pre-processing
 methods.
\end_layout

\begin_layout Subsection*
Data pre-processing
\end_layout

\begin_layout Subsubsection*

\series bold
Imputation:
\end_layout

\begin_layout Standard
Most of the common imputation methods like mean imputation and knn imputation
 did not give high scores, as expected.
 
\end_layout

\begin_layout Itemize
Mean imputation is not expected to give good results due to the high fraction
 of missing data in every column.
 
\end_layout

\begin_layout Itemize
Class-wise imputaions methods also performed badly as the domain of the
 test data is different from the domain of the train data.
\end_layout

\begin_layout Itemize
K-nearest neighbour imputation faces the issue that the 'nearest columns'
 for most of the feautres are themselves missing/imputed columns.
\end_layout

\begin_layout Standard
One method that showed better results with the above two imputation methods
 was class-wise imputation of the training data.
 But since that demands for the class labels to be known beforehand, thats
 same cannot be used on the test dataset, leading to imbalance if 'good'
 imputation for training but a 'bad' imputation for testing.
 Hence we shifted focus to methods other than classwise imputation.
\end_layout

\begin_layout Standard
Upon further reading, we found two algorithms that work well on data that
 has a large percentage of missing values - Iterative SVD imputation and
 MICE imputation.
\end_layout

\begin_layout Itemize
Iterative SVD: The method is based on singular value decomposition.
 It performs worse than classwise-knn.
\end_layout

\begin_layout Itemize
MICE: Multivariate imputation by chained equations (MICE), sometimes called
 “fully conditional specification” or “sequential regression multiple imputation
” has emerged in the statistical literature as one principled method of
 addressing missing data.
 Creating multiple imputations, as opposed to single imputations, accounts
 for the statistical uncertainty in the imputations.
 In addition, the chained equations approach is very flexible and can handle
 variables of varying types (e.g., continuous or binary) as well as complexities
 such as bounds or survey skip patterns.
\begin_inset Newline newline
\end_inset

MICE operates under the assumption that given the variables used in the
 imputation procedure, the missing data are Missing At Random (MAR), which
 means that the probability that a value is missing depends only on observed
 values and not on unobserved values.
 This assumption is suitable for the given dataset and therefore shows good
 results.
 For further reading about the implementation of MICE, refer to 
\begin_inset CommandInset href
LatexCommand href
name "this paper"
target "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3074241/"

\end_inset

.
\end_layout

\begin_layout Standard
To benchmark all the above imputation methods, logistic regression was used.
 A test train split of 80-20 for cross validation showed good results for
 knnimputation and MICE imputation, with MICE being the better one.
 knn imputation done classwise did increase the cross validation score,
 but did not perform well on the submitted test data due to the above mentioned
 reason.
 
\end_layout

\begin_layout Standard
In the case of MICE imputation, various variations were tried before we
 fixed on the final form.
 The MICE imputer takes two parameter - 
\emph on
number of imputation 
\emph default
and 
\emph on
number of nearest columns 
\emph default
to be considered.
 Benchmarking over logreg over a few parameters the final parameters fixed
 on gave - 
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

n_imputations=15
\end_layout

\begin_layout Plain Layout

n_nearest_columns=10
\end_layout

\end_inset

 which also shows that larger number of columns considered/larger number
 of imputations is not necessarily good for the evaluation metric.
\end_layout

\begin_layout Standard
The image below taken from one of the references mentioned below shows the
 variation of different kinds of error with varying amounts of missing data
 (randomly removed) of different imputation methods.
 That further reinforces the choice of MICE as an imputation method.
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Caption Standard

\begin_layout Plain Layout

\emph on
Imputation method comparison
\end_layout

\end_inset


\begin_inset Graphics
	filename figures/Mice comparison.png
	scale 50

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard

\emph on
Note:
\emph default
 While imputing, the test and the train data are clubbed together so that
 the same fit is applied for train and test, and values are filled in accordingl
y.
 This method gave very good results.
\end_layout

\begin_layout Standard

\series bold
\emph on
Observation: 
\series default
\emph default
If the first 500 features were ignored and any classifier was run on the
 remaining 2100, that gave a better score than considering the entire imputed
 dataset.
 To account for the same, feature reduction method was changed accordingly,
 as described in the following subsection.
 
\end_layout

\begin_layout Subsubsection*

\series bold
Changing the input data:
\end_layout

\begin_layout Standard
As can be seen by the distribution of values in a given feature shown below,
 the last bin of values near one shoots up and has a large bin count.
 And as can be seen by further visualisations, these values do turn out
 to be a large number of 1's - as is also shown by the feature correlation
 figure (Figure 3).
 We felt that all samples above or equal to one have been recorded as one
 in this dataset.
 
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Caption Standard

\begin_layout Plain Layout

\emph on
Distribution of samples in a feature
\end_layout

\end_inset


\begin_inset Graphics
	filename figures/Feature dist.png
	scale 40

\end_inset


\end_layout

\end_inset

Ideally, (no comments on practicality) a probability distribution should
 be fit onto this distribution and values greater than one should be placed
 at the 'correct' positions, which will be determined by the other features
 of that sample.
 Beacuse we do not know of any heuristic to find the 'correct' place to
 replace the current 1 with, we have not attempted to do so.
 Instead, we replaced all 1's with 
\emph on
nan
\emph default
s and ran imputation, the resultant dataset giving a lower fscore, as expected,
 since the information we have in the form of 1 is lost as some value between
 0 to 1.
\end_layout

\begin_layout Subsection*
Feature reduction
\end_layout

\begin_layout Standard
The feature reduction method being employed is Principal Component Analysis.
 Ths was decided due to the following reasons: 
\end_layout

\begin_layout Itemize
VarianceThreshold - choose features above a certain threshold variance value
 (to avoid almost constant features).
 In the given dataset, every feature is in the vicinity of 25% standard
 deviation, with a few ranging from 24% to 33%.
 Therefore, firstly the variance of every features is too low to successfully
 apply variance thresholding, and secondly, due to most features having
 similiar variances the method did not give good results, as expected.
\end_layout

\begin_layout Itemize
Feature Agglomeration - As analysed, some features have a very high degree
 of correlation.
 An example is shown below:
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Caption Standard

\begin_layout Plain Layout

\emph on
feature 4 vs.
 feature 2535
\end_layout

\end_inset


\begin_inset Graphics
	filename figures/f_4 vs f_2535.png
	scale 40

\end_inset


\end_layout

\end_inset

The plot also depicts the aforementioned saturation of many values at 1.
 This method showed highly promising results on cross validation data split
 of 80-20 with fscores going up to 42%, but the highest submission score
 came up to a mere 39.8%, which probably indicates overfitting on the model's
 part.
 This is one of the models that has been used in the second layer of the
 final ensemble.
\end_layout

\begin_layout Itemize
Principal Component Analysis - uses an orthogonal transformation to convert
 a set of observations of possibly correlated variables into a set of values
 of linearly uncorrelated variables.
 The difference being that in the previous approach, the 'direction of the
 dimensions' are retained, only some dimensions are dropped, whereas PCA
 determines a new set of directions for the dimension vectors.
 The unconventional method of feature reduction using PCA is described below.
\end_layout

\begin_layout Standard

\series bold
\emph on
Unique technique: 
\series default
\emph default
Based on the observation stated above, that a classification by ignoring
 the first 500 features gave a sharp increase in evaluation score, in case
 of PCA, PCA was applied separately to the first 500 features, and the remaining
 2100.
 A gridsearch over the two parameters (number of features of the first 500
 and the next 2100) gave the optimal value as being 200 from the first 500
 and 1800 form the next 2100, adding up to a total of 2000 features.
\end_layout

\begin_layout Subsection*
Classifiers
\end_layout

\begin_layout Standard
The classifiers tried out on the data are:
\end_layout

\begin_layout Itemize
LDA 
\end_layout

\begin_layout Itemize
Logistic regression
\end_layout

\begin_layout Itemize
XGboost
\end_layout

\begin_layout Itemize
Adaboost
\end_layout

\begin_layout Itemize
Random forest
\end_layout

\begin_layout Itemize
SVMs
\end_layout

\begin_layout Itemize
Neural networks
\end_layout

\begin_layout Standard
Starting off with logreg as a benchmark, LDA gave very poor results (due
 to its strong assumptions of data distribution).
 The best results were given by SVMs.
\end_layout

\begin_layout Standard

\series bold
\emph on
Observation: 
\series default
\emph default
Bagging reduces the fscore, for any given model.
 Since bagging tackles high variance/overfitting of data, this implies that
 the dataset has an already low variance and bagging will not boost the
 performance.
 Therefore, the complementary choice to implement boosting instead of bagging
 led to to the usage of xgboost and adaboost.
 Touted to be the wininng algorithm for Machine Learning contests, even
 xgboost did not give good results which indicates that there is no high
 bias existing either.
 
\end_layout

\begin_layout Subsubsection*

\emph on
Support Vector Machines
\emph default
: 
\end_layout

\begin_layout Standard
With the Kernels coming to excellent use, SVMs gave progressivley higher
 scores with better and better data pre-processing techniques.
 As mentioned above, the 'best' dataset with 200 features from the first
 500 and 1800 from the next 2100 was used to run a gridsearch to find the
 best parameters on cross validation for 
\emph on
sigmoid
\emph default
 and 
\emph on
rbf 
\emph default
kernels.
 Below are the results for both the grid searches:
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Caption Standard

\begin_layout Plain Layout

\emph on
rbf kernel gridsearch
\end_layout

\end_inset


\begin_inset Graphics
	filename figures/RBF.png
	scale 50

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Caption Standard

\begin_layout Plain Layout

\emph on
sigmoid kernel gridsearch
\end_layout

\end_inset


\begin_inset Graphics
	filename figures/Sigmoid.png
	scale 50

\end_inset


\end_layout

\end_inset

which led us to fixing the parameters of SVMs at 
\emph on
c =10 and gamma = 0.01 
\emph default
and the kernel as 
\emph on
rbf.
 
\emph default
The other parameter that could have varied a lot of results was the choice
 betweek one versus all and one versus rest.
 Given below are the class sizes:
\begin_inset Float table
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Caption Standard

\begin_layout Plain Layout

\emph on
Class sizes
\end_layout

\end_inset


\begin_inset Tabular
<lyxtabular version="3" rows="9" columns="8">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Class
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Size
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Class
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Size
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Class
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Size
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Class
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Size
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
403
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
9
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
277
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
17
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
480
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
25
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
346
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
311
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
10
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
379
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
18
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
382
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
26
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
150
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
3
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
394
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
11
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
333
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
19
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
474
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
27
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
340
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
4
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
153
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
12
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
254
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
20
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
220
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
28
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
351
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
5
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
304
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
13
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
130
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
21
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
468
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
29
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
176
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
6
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
472
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
14
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
271
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
22
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
425
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
7
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
252
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
15
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
258
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
23
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
436
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
8
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
243
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
16
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
336
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
24
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
483
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\end_inset

which indicates that one vs rest may lead to class imbalance while evaluating
 its hyperplanes.
 But that is not the case as both are seen to give similar cross validation
 scores, with one vs one just taking a longer computational time as expected.
 
\end_layout

\begin_layout Standard
Each of the above combinations was tried for differently pre-preprocessed
 data, giving the 
\emph on
best submission model
\emph default
 with the MICE imputed (200+1800) PCA feature reduced dataset.
\end_layout

\begin_layout Standard
The other form of feature reduction - Feature Agglomeration - gave promising
 results on cross validation (42% fscore) but the final submission coming
 only up to 39.8%, implying overfitting of the train and test.
\end_layout

\begin_layout Subsubsection*

\emph on
Neural Networks: 
\end_layout

\begin_layout Standard
Neural networks: To fit high dimensional data, neural networks are a very
 good choice.
 We explored deep networks with different depths and neuron counts.
 We also varied the minibatch size, learning rate and optimiser functions.
 The three models out of all that we tried that has given us the best insights
 are explained below.
\end_layout

\begin_layout Itemize

\series bold
Model1: 
\series default

\begin_inset Newline newline
\end_inset

Input: Hidden layers: 2000,1000,500.
 
\begin_inset Newline newline
\end_inset

Learning rates: 0.001, 0.0001 
\begin_inset Newline newline
\end_inset

Optimiser: Adam
\begin_inset Newline newline
\end_inset

Observation: The model overfit early on giving a max validation score of
 33.39
\end_layout

\begin_layout Itemize

\series bold
Model2: 
\series default

\begin_inset Newline newline
\end_inset

Input: 
\begin_inset Newline newline
\end_inset

Hidden layers: 1500,500,200 
\begin_inset Newline newline
\end_inset

Learning rates: 0.001, 0.0001 
\begin_inset Newline newline
\end_inset

Optimiser: Adam 
\begin_inset Newline newline
\end_inset

minibatch size: 1000 
\begin_inset Newline newline
\end_inset

Observation: The model again overfit early on giving a max validation score
 of 33.39 The training loss decreae was much smoother than with smaller minibatch
 sizes previously.
\end_layout

\begin_layout Itemize

\series bold
Model3: 
\series default

\begin_inset Newline newline
\end_inset

Input: 2000D (200 best features(PCA) of first 500 from mice imputation,
 first 1800(PCA) from the rest of 2100 features) 
\begin_inset Newline newline
\end_inset

Hidden layers: 800, 300 
\begin_inset Newline newline
\end_inset

Dropout (keep_probab) values tried: 0.9, 0.7, 0.5, 0.2
\begin_inset Newline newline
\end_inset

Learning rates: 0.001 with step descent after 10000 iterations
\begin_inset Newline newline
\end_inset

Optimiser: Adam
\begin_inset Newline newline
\end_inset

Observation: We tried to introduce batch_norm after each layer, but the
 training almost saturated very early making incremental updates, hence
 we removed it.
 The learning was much faster with keep_probab = 0.9, but the test loss hit
 a minimum at test accuracy = 34.78 The best validation accuracy of 37.4 was
 obtained with keep_probab = 0.5 With keep_probab = 0.2, the model was unable
 to overfit completely, the training accuracy reached only a maximum of
 0.83 after 100000 iterations.
 
\end_layout

\begin_layout Standard
The training and test loss of the first 10000 iterations of Model 3 with
 dropout (
\emph on
0.5
\emph default
) is shown below:
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Caption Standard

\begin_layout Plain Layout

\emph on
green - test loss; red - train loss
\end_layout

\end_inset


\begin_inset Graphics
	filename figures/figure_1.png
	scale 45

\end_inset


\end_layout

\end_inset

The evaluation metrics are understandably low due to overfitting by the
 above models, which can be depicted by the following TSNE plots:
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Caption Standard

\begin_layout Plain Layout

\emph on
Initial TSNE plot
\end_layout

\end_inset


\begin_inset Graphics
	filename figures/initialTSNE.png
	scale 50

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Caption Standard

\begin_layout Plain Layout

\emph on
TSNE plot - output of neural network
\end_layout

\end_inset


\begin_inset Graphics
	filename figures/nn_overfit.png
	scale 50

\end_inset


\end_layout

\end_inset

As can be seen, the data points that belong to the same class are separated
 out into smaller separated clusters which is due to overfitting.
\end_layout

\begin_layout Subsubsection*

\emph on
XGBoost: 
\end_layout

\begin_layout Standard
Due to bagging not giving a good result, the counterpart boosting was tried.
 The tunable parameters are: 
\emph on
eta, max_depth, number of iterations.

\emph default
 With a cross validation score of 34%, this model was used in the final
 weighted voting ensemble.
\end_layout

\begin_layout Standard
The two kinds of parameter variations gave the following results:
\end_layout

\begin_layout Itemize

\emph on
max_depth = 2 (stub), max_iterations = 200 
\emph default
- gave 34.7% fscore on validation
\end_layout

\begin_layout Itemize

\emph on
max_depth
\emph default
 = 1
\emph on
(root), max_iterations = 300
\emph default
 - training fscore saturated at 62%, while validation fscore gave 27%
\end_layout

\begin_layout Itemize

\emph on
max_depth = 10, max_iterations = 50 
\emph default
- gave 0.98 fscore on train and 27.2% on test (implying overfitting)
\end_layout

\begin_layout Itemize

\emph on
early_stopping
\emph default
 - was set to 1/10th of the max_iterations which kept track of the increment
 in validation score and terminates when the model does not improve over
 the specified iterations
\end_layout

\begin_layout Subsubsection*

\emph on
Random forest: 
\end_layout

\begin_layout Standard
Was implemented to try and overfit the data, since bagging gave no good
 results.
 Increasing the number of estimators and depth of the model just saturated
 the fscore at around 31%, even with variation in the number of features
 being randomly sampled by each tree.
\end_layout

\begin_layout Subsubsection*

\emph on
K-nearest neighbours:
\end_layout

\begin_layout Standard
We ran KNNs over k = 2 to k = 30.
 There wasn't a significant increase in score with increasing k.
 All values of k gave less than 24% fscore.
\end_layout

\begin_layout Subsection*
Ensemble
\end_layout

\begin_layout Standard
The idea of ensembling classifiers is to use outputs of weak classifiers
 and cumulatively generate a better output.
 We used two layers of ensembling the classifiers.
 In the first layer, we used the outputs from:
\end_layout

\begin_layout Enumerate
Neural networks:
\begin_inset Newline newline
\end_inset

i.
 With dropouts - (3 models)
\begin_inset Newline newline
\end_inset

ii.
 Without dropouts - (1 model)
\end_layout

\begin_layout Enumerate
SVM: Every model used random 80% of the data, since bagging caused reduction
 in performance of the model.
 Since we were fairly confident about the hyperparameters to get the best
 SVM, we had to sample such distributions to bring in variation.
\begin_inset Newline newline
\end_inset

i.
 rbf: c =10, gamma = 0.01 on PCA reduced dataset (20 models)
\end_layout

\begin_layout Enumerate
XGBoost
\end_layout

\begin_layout Standard
Three weak models each with a score of around 35%, form the first layer
 of ensemble.
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
medskip
\end_layout

\end_inset


\end_layout

\begin_layout Standard
The output of the first layer of ensemble was then used for the second layer,
 in which the other participant classifers were given weighted vote based
 on their fscores:
\end_layout

\begin_layout Enumerate
SVM - C=10, gamma=0.01, on MICE imputed PCA reduced data (200+1800 as mentioned
 above) 
\emph on
(the second submitted model with the highest fscrore)
\end_layout

\begin_layout Enumerate
SVM - C=10, gamma = 0.01, on MICE imputed Feature Agglomeration reduced data
 (data in which correlated features have been clustered together) with 
\emph on
n_clusters = 2000
\end_layout

\begin_layout Enumerate
Output of previous layer
\end_layout

\begin_layout Standard
This the the first model that has been submitted as a 'final submission.
\begin_inset Newline newline
\end_inset

The second model submitted is :- SVM - C=10, gamma=0.01, on MICE imputed
 PCA reduced data (200+1800 as mentioned above)
\end_layout

\begin_layout Section*
References
\end_layout

\begin_layout Standard
[1] fancyimpute
\begin_inset Newline newline
\end_inset

[2] MICE - https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3074241/
\begin_inset Newline newline
\end_inset

[3] iSVD - https://www.ncbi.nlm.nih.gov/pubmed/11395428
\begin_inset Newline newline
\end_inset

[4] MICE - https://www.omicsonline.org/open-access/a-comparison-of-six-methods-for
-missing-data-imputation-2155-6180-1000224.pdf
\begin_inset Newline newline
\end_inset

[5] sklearn documentation
\begin_inset Newline newline
\end_inset

[6] tensorflow documentation
\end_layout

\begin_layout Section*
Closing Remarks:
\end_layout

\begin_layout Standard
We were glad to work on a real Machine Learning contest (with synthetic
 data) exploring the different classifiers and their uniqueness.
 We also understood the need for data exploration and analysis, which is
 just as important as building powerful models.
 We learnt the importance of having a proper schedule and working in a structure
d way.
 We learnt to colloborate online using Git which saved us a lot of time.
 DISCLAIMER: Everything in this report is true to the core, except the last
 line.
 We used Gmail.
\end_layout

\end_body
\end_document
